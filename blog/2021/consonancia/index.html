<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> a distill-style blog post | You R. Name </title> <meta name="author" content="You R. Name"> <meta name="description" content="an example of a distill-style blog post and main elements"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://elacosse.github.io/blog/2021/consonancia/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "a distill-style blog post",
            "description": "an example of a distill-style blog post and main elements",
            "published": "May 22, 2021",
            "authors": [
              
              {
                "author": "Eric Lacosse",
                "authorURL": "https://elacosse.github.io",
                "affiliations": [
                  {
                    "name": "Champalimaud Foundation, Lisbon, Portugal",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"> <span class="font-weight-bold">You</span> R. Name </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>a distill-style blog post</h1> <p>an example of a distill-style blog post and main elements</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#conson%C3%A2ncia-its-conceptualization">ConsonâncIA - its conceptualization</a> </div> <div> <a href="#human-machine">Human-Machine</a> </div> <div> <a href="#human-human">Human-Human</a> </div> <div> <a href="#human-self">Human-self</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> <div> <a href="#footnotes">Footnotes</a> </div> <div> <a href="#citations">Citations</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>What does the future of treating human health look like in a world where humans are not the only ones talking and deciding? How would we ensure that this future is one we want to live and be medically treated in?</p> <p>ConsonâncIA is an immersive audiovisual work that was hosted at Metamersion, an event that takes place at the Champalimaud Foundation in Lisbon, Portugal. The event combines perspectives from science, multimedia art, technology, and clinical healthcare to build immersive art-science exhibitions for the public and research community to experience. It has found its way to a third edition, growing from a small, improvised event to becoming an integral part of forming a wider initiative to establish the first Digital Therapeutics Center in the world. For three days (May 17th - 19th, 2024), Metamersion ran to display prototype experiences and art works, inspired by ongoing research developed from the Neuroscience program of the Champalimaud Centre for the Unknown, and how work coming primarily from fundamental neurosciences could bridge to clinical applications as digital therapeutics, i.e., “software as medicine.” These installations leaned heavily on methodology from generative AI, computational neuroscience and other innovative technologies to generate new multimedia experiences. The public was also able to participate in immersive performance art, roundtable discussions, and multimedia concerts.</p> <p>This year’s Metamersion theme was “Healing Algorithms” and featured more installations than all previous years combined. What follows below are details about why and how the piece was conceptualized and what technical elements ultimately went into building it. Here you will find a “behind-the-scenes” look into what kind of thinking and process went into putting something like it together, from raw conceptualization to the experience displayed to the public.</p> <h2 id="consonância---its-conceptualization">ConsonâncIA - its conceptualization</h2> <p>Consonância (Portuguese), or consonance (English), refers to a state of agreement or harmony between different elements in a system. ConsonâncIA was designed to provide an immersive experience at the intersection of machine and human expression and intelligence, encouraging visitors to imagine what form a healing experience of the future would take. That is, if machines could capture and mediate individual expressions of hurt or suffering, and ultimately work to help alleviate and heal from it. The essence of the word consonância tries to capture how machine intelligence aligning with an individual’s desire for their own well-being could facilitate a healing experience that would bring about better consonance between others and ultimately themselves in the careful, intentional ways we need to think about in designing and deploying digital therapeutics.</p> <p>Overall, the motivation for ConsonâncIA is captured by themes of modern AI alignment research, emphasizing the need for AI systems that are in consonance with human values, their diverse ways of thinking, and perhaps most importantly, through fostering connections with others mediated through real empathy and mutual understandings, a feature current AI systems will completely lack for the foreseeable future. Achieving these objectives could ultimately contribute to our individual and collective well-being, the underlying goal of serving digital therapeutics.</p> <p>AI alignment, broadly speaking, refers to the challenge of ensuring how increasingly capable AI systems can be steered to become trusted, aligning and respecting human values and goals. We, as architects of AI systems, need to solve the problem of how to create the robust mechanisms for these systems to act beneficially, in accordance with our societal ethics and preferences at large, i.e., a collective good. ConsonâncIA takes a broader view of the objectives present in alignment research that typically focuses on an isolated set of goals of individual humans and machines; instead, we as AI system designers need to take into consideration tractable goals that define alignment between other humans, i.e., human-human, and ultimately the alignment defining one’s values/goals internally, i.e., human-self. Alignment objectives, emphasizing the context of deploying digital therapeutics, would be better served if we adopt this more holistic view accommodating human-human and human-self aspects into the alignment problem. These two objectives are often understood occupying a realm outside those of machines reaching in or mediating. ConsonâncIA demonstrates how these objectives can be considered together within the general AI alignment problem.</p> <p>The following figure provides the conceptual basis through which ConsonâncIA with its three distinct elements should be seen – machine, human, self – and constitute what we refer to as the alignment triad. Each part of the immersive audiovisual work provided a distinct experience that corresponded to one of these three elements of this triad where the underlying interaction focus of the experience was between either machine, human, or self.</p> <div class="mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/consonancia/triad-480.webp 480w,/assets/img/consonancia/triad-800.webp 800w,/assets/img/consonancia/triad-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/consonancia/triad.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h2 id="machine-human">Machine-human</h2> <p>The initial task was to gather information from human participants at the installation with the intention of building a cognitive map of their unique experience with hurt. This should be done in a way respects their autonomy or willingness to share information. Further, how do we perform rich and accurate human cognitive mapping without relying on expensive and taxing neuroimaging or controlled behavioral experimentation? We utilize conversational language agents to collect information from humans at scale. Conversational Language Agents (CLA), also known as chatbots or dialogue systems, are AI systems designed to engage in human-like conversations. The main goal of CLAs are to provide subjects with an experience modeled from a natural interaction, akin to how one would communicate with another person. It is an exciting research direction to understand how CLAs should carry out conversations with specific, underlying goals of collecting self-reported data from humans in the domains we wish to map. Conventional approaches, like surveys or forms, rely on highly constrained preconceptions of what information should be asked and remain limited for that reason. In understanding pain, for instance, most clinical studies have highly constrained notions of what qualities pain manifest. This is a major reason why pain is so difficult to examine and treat. The lack of clear, objective measures, its multifactorial nature holding both complex cognitive and physical components, and the subjective nature of describing pain all make it extremely difficult for clinicians to deal with patient symptoms appropriately.</p> <p>This is where inspiration from different research methodologies common within the humanities may enable new kinds of science to be conducted and why CLAs may offer new means of achieving new results in domains where research has stifled. Qualitative research methodology focuses on understanding human experience from the perspective of those who live them. The field of phenomenology, originally rooted in early 20th century work following the philosophy of Edmund Husserl, is a well-established approach that at its core focuses on exploring and describing people’s lived experienced as reported from their own perspective and words. It primarily follows from conducting in-depth interviewers and observations to gather common themes and structures within an individual’s lived experience. These data largely follow from self-report gathered by human interviewing. Characteristic in these investigations is the focus of collecting data in naturalistic settings where participants engage in contexts of their normal, everyday activities. Researchers are able to observe behaviors and interactions in their real-life contexts and environments as opposed to the highly constrained laboratory ones. Although conventional laboratory investigations yield greater control, they also require researchers to ignore dimensions to an individual’s own experience that fall outside of preconceived notions of the experimenter. Phenomenological investigations hope to gather additional information that help structure and improve upon any preconceived notions that an experimenter may have or are simply limited to because of the particular environments and structure demanded from these investigations in typical laboratory settings.</p> <p>So far, rule-based chatbots of the past have not been able to generate the depth of conversation necessary to leverage phenomenological approachs. These older chatbots quickly exhaust possible conversational flows. Most qualitative research investigations necessitate nuanced intuitions from humans conducting interviewers to understand what questions and conversation flows are needed to engage subjects such that they gather necessary and relevant information in a way that emulates the methodology a human would. Newer machine learning methodology has greatly change our ability to design CLAs that rival even human abilities; pre-trained language models, i.e., Large Language Models (LLMs), have enabled powerful text generation capabilities that continue to improve in terms of their human-like ability to hold conversations that accord with human conversational nuance and intuitions. These models are mainly based on large transformer architectures that rely on pre-training over massive amounts of text corpora generated by humans. They also rely heavily on further fine-tuning via preference alignment techniques like Reinforcement Learning with Human Feedback (RLHF).</p> <p>CLAs are not only limited to interaction with text, however. For ConsonâncIA, we incorporated additional capabilities for the CLA to understand aspects of the subjects external environment by allowing them to send images to the CLA. This ability was made possible by Vision Language Models (VLMs), i.e., multimodal models that learn from both images and text. These models have shown promising zero-shot ability and generalize across most image domains. Additionally, subjects, if they chose so, could interact with the CLA via speaking to it. Speech audio was transcribed using the latest Speech-To-Text (STT) models.</p> <p>Advancements in generative AI have not only greatly enhanced text generation capabilities, but have also paved the way for the development of generative image models, such as DALL-E or and Stable Diffusion. In this exploration, we created a novel form of conversational narrative. It harnessed the power of these generative models through enriching the conversation by integrating them seamlessly with generative images. The motivation for this was multifold and mainly sought to aid in capturing rich, descriptive experiences from individuals. Images possess the power to evoke strong emotional responses, adding a layer of complexity words alone cannot capture. Memories often surface when we encounter an image, as it can trigger a familiar scene, object, place, or experience, inviting us to navigate between spaces of recollection and imagination. These phenomena enhance the CLA cognitive mapping objectives. Furthermore, the inclusion of generative images engages the subject in a more profound interpretation and interaction with the conversation. By incorporating images into a conversational narrative, a certain depth and meaning can transcend words, blurring the boundaries between the past, present, and imagined states. These aspects together produced a narrative that enhanced the normal (and rather dull) conversational narratives individuals have on their mobile phones.</p> <div class="mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/consonancia/CLA-480.webp 480w,/assets/img/consonancia/CLA-800.webp 800w,/assets/img/consonancia/CLA-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/consonancia/CLA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"elacosse/elacosse.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 You R. Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>